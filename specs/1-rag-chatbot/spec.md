# Feature Specification: RAG Chatbot for Physical AI Book

**Feature Branch**: `1-rag-chatbot`
**Created**: 2025-12-06
**Status**: Draft
**Input**: User description: "Feature: RAG Chatbot for Physical AI Book Purpose: Build an intelligent assistant integrated into the online book that answers questions strictly using the book's content. Requirements: - Ingest all Markdown files under /web/docs as the knowledge base. - Chunk text into 500–800 token passages with overlap. - Create embeddings using OpenAI (or chosen model). - Store embeddings in Qdrant Cloud (Free Tier). - Store metadata & conversation logs in Neon Postgres. - API backend must expose: - POST /v1/query - POST /v1/query-with-selection - POST /v1/feedback - GET /v1/conversation/{id} - Create retrieval using vector similarity search. - Support “Answer only from selected text” mode. - Provide source citations for all answers. - Use ChatKit/Agents SDK to generate final answers. - Build a web-based chat interface with: - message history - typing indicator - selection-based question mode - All code must be generated by Claude Code."

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Ask Questions and Get Cited Answers (Priority: P1)

A user browsing the Physical AI book wants to ask a question about the content. The chatbot should provide a relevant and concise answer, strictly based on the book's information, and include citations to the source Markdown files.

**Why this priority**: This is the core functionality and provides immediate value to the user by making the book's content interactively accessible.

**Independent Test**: Can be fully tested by asking a question related to the book's content (e.g., "What is ROS2?") and verifying that the chatbot returns an accurate answer with correct citations.

**Acceptance Scenarios**:

1.  **Given** a user is on any page of the online book, **When** they type a question into the chatbot widget and submit it, **Then** the chatbot displays a relevant answer derived solely from the book's content.
2.  **Given** the chatbot has provided an answer, **When** the user reviews the response, **Then** the response includes clear citations (e.g., "Source: docs/module1-ros2.md") for all information provided.
3.  **Given** a user asks a question whose answer is not present in the ingested book content, **When** the chatbot processes the query, **Then** the chatbot explicitly states that it cannot find the answer within the book's knowledge base.

---

### User Story 2 - Query with Selected Text Context (Priority: P2)

A user wants to ask a highly specific question related to a particular section of the book. They select a piece of text on the page and then ask a question, expecting the chatbot to use *only* the selected text as its primary context for generating the answer.

**Why this priority**: This enhances the precision of the chatbot's responses and allows for deeper, context-specific inquiries, improving the overall utility for detailed research.

**Independent Test**: Can be fully tested by selecting a paragraph from a content page, asking a question directly related to that paragraph (e.g., "Explain this concept"), and verifying the chatbot's answer is based *only* on the selected text and cites it.

**Acceptance Scenarios**:

1.  **Given** a user selects a segment of text on a content page, **When** they activate the selection-based question mode and submit a query, **Then** the chatbot generates an answer using only the content from the selected text, even if broader information exists elsewhere.
2.  **Given** the chatbot responds to a selection-based query, **When** the user reviews the response, **Then** the response cites the source of the selected text and strictly avoids citing other parts of the book unless explicitly relevant and within the selected scope.

---

### User Story 3 - Review Conversation History (Priority: P3)

A user wants to review their previous interactions with the chatbot to recall information or continue a previous line of inquiry.

**Why this priority**: This improves user experience by providing continuity and reducing the need to re-ask questions, making the chatbot more useful over time.

**Independent Test**: Can be fully tested by interacting with the chatbot, closing and reopening the browser/widget, and verifying that previous messages are still visible or retrievable.

**Acceptance Scenarios**:

1.  **Given** a user has previously interacted with the chatbot, **When** they open the chatbot widget, **Then** their prior conversation messages are displayed, maintaining context.
2.  **Given** a user's conversation history is loaded, **When** they scroll through it, **Then** all previous queries and chatbot responses are visible and correctly ordered.

---

### Edge Cases

- **Out-of-Scope Questions**: What happens when a user asks a question completely unrelated to the Physical AI book content? (Chatbot should politely state it cannot answer and refer to its purpose).
- **Ambiguous Queries**: How does the system handle vague or poorly phrased questions? (Chatbot should attempt a best-effort retrieval, or ask for clarification if severely ambiguous).
- **Empty Knowledge Base**: What if no Markdown files are ingested, or the ingestion process fails? (Chatbot should report an error or inability to answer questions).
- **Very Long Selected Text/Queries**: How does the system handle extremely long user queries or large blocks of selected text (e.g., exceeding token limits)? (System should gracefully handle truncation or provide a warning).
- **No Relevant Chunks**: What if vector search returns no relevant chunks for a given query? (Chatbot should respond with "I don't know" or similar).

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST ingest all Markdown files located under the `/web/docs` directory of the content management system as its primary knowledge base.
- **FR-002**: System MUST process ingested text by chunking it into passages with a length between 500 and 800 tokens, ensuring a specified overlap between consecutive chunks to preserve context.
- **FR-003**: System MUST generate high-quality vector embeddings for each text chunk using an embedding service.
- **FR-004**: System MUST store the generated text chunk embeddings and associated metadata (e.g., source file path, chunk content, token range) in a vector database.
- **FR-005**: System MUST store user conversation logs (queries, responses, timestamps, user IDs) and any other necessary application metadata in a relational database.
- **FR-006**: The API backend MUST expose a `POST /v1/query` endpoint that accepts a user's natural language question and returns a generated answer with source citations.
- **FR-007**: The API backend MUST expose a `POST /v1/query-with-selection` endpoint that accepts a user's natural language question and a block of selected text, and returns an answer strictly grounded in the selected text, with source citations.
- **FR-008**: The API backend MUST expose a `POST /v1/feedback` endpoint to allow users to provide feedback on chatbot responses (e.g., thumbs up/down, brief comment).
- **FR-009**: The API backend MUST expose a `GET /v1/conversation/{id}` endpoint to retrieve the full history of a specific user's conversation.
- **FR-010**: System MUST implement a retrieval mechanism that uses vector similarity search on the vector database to find the most relevant text chunks for a given query or query-with-selection.
- **FR-011**: System MUST ensure that responses generated in the "query-with-selection" mode are strictly limited to the information present in the provided selected text, explicitly stating if the answer cannot be found within that specific context.
- **FR-012**: System MUST automatically extract and provide exact source citations (e.g., `docs/chapter.md:line_start-line_end`) for all information presented in chatbot answers, linking back to the original Markdown files.
- **FR-013**: System MUST utilize a AI agent framework-compatible method for orchestrating the RAG pipeline and generating the final natural language responses based on retrieved context.
- **FR-014**: The web-based chat interface MUST display a clear and chronological message history of the user's interaction with the chatbot.
- **FR-015**: The web-based chat interface MUST display a visual typing indicator when the chatbot is actively processing a query and generating a response.
- **FR-016**: The web-based chat interface MUST incorporate a user interface element that allows users to easily provide selected text as additional context for their queries.

### Key Entities *(include if feature involves data)*

-   **Document Chunk**: Represents a segment of text from a Markdown file.
    -   Attributes: `id`, `content` (text of the chunk), `embedding` (vector representation), `source_file` (path to original Markdown), `start_line`, `end_line`, `timestamp` (when chunk was processed).
-   **Conversation**: Represents a user's interaction session with the chatbot.
    -   Attributes: `conversation_id`, `user_id` (if authenticated), `timestamp_started`, `timestamp_last_message`.
-   **Message**: Represents a single query or response within a conversation.
    -   Attributes: `message_id`, `conversation_id`, `sender` (`user` or `bot`), `text_content`, `timestamp`, `sources` (list of cited document chunks for bot messages), `selected_text` (if user query included selection).
-   **Feedback**: Represents user feedback on a bot's message.
    -   Attributes: `feedback_id`, `message_id`, `user_id`, `rating` (e.g., positive/negative), `comment`, `timestamp`.

## Success Criteria *(mandatory)*

### Measurable Outcomes

-   **SC-001**: For 90% of user queries, the chatbot provides a relevant and accurate answer with citations within 5 seconds from query submission.
-   **SC-002**: 100% of chatbot responses to questions answerable by the book content include at least one correct source citation.
-   **SC-003**: When using the "query-with-selection" mode, 95% of responses strictly adhere to the provided selected text, and for the remaining 5%, the chatbot explicitly states if it cannot answer from the selected text.
-   **SC-004**: The web-based chat interface successfully loads and displays within 2 seconds on any content page for 95% of users.
-   **SC-005**: For questions outside the scope of the Physical AI book's content, the chatbot correctly identifies and responds with "I don't know" or a similar statement for 90% of such queries.
